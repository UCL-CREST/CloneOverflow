% Copyright Javier SÃ¡nchez-Monedero.
% Please report bugs and suggestions to (jsanchezm at uco.es)
%
% This document is released under a Creative Commons Licence 
% CC-BY-SA (http://creativecommons.org/licenses/by-sa/3.0/) 
%
% BASIC INSTRUCTIONS: 
% 1. Load and set up proper language packages
% 2. Complete the paper data commands
% 3. Use commands \rcomment and \newtext as shown in the example

\documentclass[a4paper,twoside,10pt]{reviewresponse}

% 1. Load and set up proper language packages
%\usepackage[utf8x]{inputenc}
\usepackage{newtxtext}
\usepackage{newtxmath}
\usepackage[latin9]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{float}
\usepackage{listings} 
\usepackage{multirow}
\usepackage{booktabs} % For formal tables

\lstset{language=Java,
	tabsize=2,
	basicstyle=\ttfamily\footnotesize,
	mathescape=true
} 

% 2. Complete the paper data
\newcommand{\myAuthors}{Chaiyong Ragkhitwetsagul$^\ast$,~Jens Krinke$^\dagger$,~Matheus Paixao$^\dagger$,~Giuseppe Bianco$^\star$,~Rocco Oliveto$^\star$}
\newcommand{\myAuthorsShort}{Ragkhitwetsagul et. al}
%\newcommand{\myEmail}{cragkhit@gmail.com, {j.krinke,matheus.paixao.14}@ucl.ac.uk, giuseppebianco92@gmail.com, rocco.oliveto@unimol.it}
%\newcommand{\mySecEmail}{}
\newcommand{\myTitle}{Cover Letter for the Reviewers of the Paper \\ ``Toxic Code Snippets on Stack Overflow''}
\newcommand{\myShortTitle}{Cover Letter}
\newcommand{\myJournal}{IEEE TRANSACTIONS ON SOFTWARE ENGINEERING}
\newcommand{\myDept}{Mahidol University (Thailand)$^\ast$, University College London (UK)$^\dagger$, University of Molise (Italy)$^\star$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\usepackage[linktoc=all]{hyperref}
\usepackage[linktoc=all,bookmarks,bookmarksopen=true,bookmarksnumbered=true]{hyperref}

\hypersetup{
pdfauthor = {\myAuthorsShort},
pdftitle = {\myTitle},
pdfsubject = {\myJournal\xspace},
colorlinks = true,
linkcolor=blue,          % color of internal links
citecolor=black!70!green,        % color of links to bibliography
filecolor=magenta,      % color of file links
urlcolor=black           % color of external links
}

\newcommand\FIXME[1]{{\color{red}\textbf{FIXME: #1}}}

\begin{document}

\thispagestyle{plain}

\begin{center}
 {\LARGE\myTitle} \vspace{0.3cm} \\
 {\large\myJournal} \vspace{0.3cm} \\
 \today \vspace{0.3cm} \\
 \myAuthors \\
% \url{\myEmail} \\
 %\url{\mySecEmail} 
 \vspace{0.3cm} 
 \myDept \vspace{1cm}
\end{center}

%\tableofcontents

%\begin{abstract}

This paper is a journal-first submission to Transactions on Software Engineering. This is a minor revision to the submission no.~TSE-2018-06-0228 and we would like to thank the anonymous reviewers for taking their time to read our paper carefully. With their useful and constructive reviews, we improved the work further as listed below and as discussed later in this letter.

\begin{enumerate}
\item We have softened the claims as suggested by Reviewer~3.
\item While we have considered updating the dataset, we chose not to do so as that would require to remove an important part of our study, the evolution of the identified outdated code snippets. Moreover, we don't expect that an update would change the results because Stack Overflow answers are rarely modified.
\item All other points regarding the experimental method have been addressed carefully and completely.
\end{enumerate}

We tried our best to address all the comments from the reviewers as discussed in the following.
        

%Lastly, we would also like to ask the reviewers to comment on how to address the
%newly established page limits of IEEE TSE that was enforced on 18 February 2018, after our previous
%submission (15 November 2017). After incorporating the suggestions from the reviewers and
%rewriting, our paper is now extended to 22 pages. Are there any elements that
%could or should be removed, or should a split into two separate papers be
%considered?

\clearpage

\section{Reviewer 1}

\rcomment{The paper presents an interesting empirical analysis of what the authors refers to as toxic code clones on Stack Overflow. The paper presents a study using state-of the art clone detection tools and techniques to automatically extract and filter online clone pairs between Stack Overflow and projects in the curated Qualitas corpus. The paper also includes a user survey conducted on Stack Overflow users to assess their awareness of such potential toxic code fragments.he paper is well organized,  sound and the authors spend amount of work in their empirical analysis.
\vspace{0.2cm}
	
The authors did a very good job in explaining their methodology and the results they obtained from their user survey and the online clone detection experiment.
\vspace{0.2cm}

Probably the least convincing part of the paper is the study on toxic license violations. This limitations however is partially due to the grey zone these licenses proliferations have remained in. Even license violation detection tools only cover a subset of all possible license proliferations which might exist. This is a general issue for all open source developers and not limited to code reuse through Stack Overflow. Authors do acknowledge in their discussion this challenge, due to different legislation in different countries and their goal to increase the SO user awareness of this potential violations is welcome addition.
\vspace{0.2cm}

Threats to validity in terms of generalizability will basically always exist and the authors did a good job in mitigating these challenges or acknowledging the limitations of their studies.
\vspace{0.2cm}

Section 3.6.2 Actionable items is a good start to drive further research in this area. However, a general concern is that the potential overall impact of these toxic code snippets seems to be rather small in terms of number of posts on SO or even in terms of code reuse from SO. Should we really care about them? But then again, I guess any improvement to the overall quality of software is a desirable contribution.}

We would like to thank you for your insightful comments regarding the license violations. As you have mentioned, software license violations are difficult to identify and validate due to the large number of software licenses, differences in countries' legislation, and the amount of code to be considered copyrighted (vs.~fair use). Nevertheless, the case of copying a licensed code snippet from an open source project to Stack Overflow without its license shows a clear potential for violating the project's license. By detecting such case, we can alleviate the license issues by highlighting or stopping the accidental conversion of strictly licensed code (such as GPL) to permissively licensed code (such as CC BY-SA 3.0 on Stack Overflow). As we highlight in the Introduction, Page~2 (and Footnote~2), developers are worried about copyright issues. We hope that our study will increase awareness on this issue to Stack Overflow and the developers using Stack Overflow.

Regarding the few number of toxic code snippets in SO posts and code reuse in GitHub, this is partially due to the data set of (only) 111 Java projects from the Qualitas corpus. The clones (and the discovered outdated code and potentially license-violating code snippets) reported in this paper are only subject to these 111 projects. The number may increase if we expand the amount of open source projects. Moreover, this study only focuses on Java, and the situation may be worse or better for other programming languages. We have added this discussion into the Threats to Validity. \FIXME{Done}

\rcomment{
\textbf{Minor issues}
\vspace{0.2cm}

Page 17, Col.1, L14--15
As shown in Table 23, the clones were found in highly starred projects (???  29,465 to 10 stars ???) to 1-star projects. Please rephrase
}

We rephrased the sentence to ``As shown in Table 23, the clones were found in highly-starred projects (29,465 stars) to lowly-starred star projects (1 star).''

\rcomment{
Page 17, Col.2, L11
Software auditing services such as Black Duck Software or nexB, which can effectively check for ...
Please provide the Reference for Black Duck Softwqre or nextB -- you have only later on Page 18
}

Thank you for the comment. We have moved the references for Black Duck Software and nextB to their first appearance on Page 17.

\section{Reviewer 2}

\rcomment{Thank you to the authors for the revised manuscript. I think the authors did a great job addressing most comments, specifically, R2C2, C6, C7, C9 and C10.
\vspace{0.2cm}

That said, I still would like more clarification on some of the other comments. My detailed comments are below:
\vspace{0.2cm}

R2C1: I am not sure the new definition fits well within your study context. It was more about technical debt and I would remove that definition and stick to the next sentence where you specifically say that toxic code snippets are snippets that are 1) outdated, ...etc.}

Thank you for the suggestion on the definition of toxic code snippets. Since Reviewer~3 also expresses his/her concern regarding the definition, we revised the definition by removing the technical debt part and shorting it as follows:

``Toxic code snippets mean code snippets that, after incorporating into software, degrade the software quality.
Stack Overflow code snippets
cloned from open source software or online sources can become toxic when they
are (1)~outdated, (2)~violating their original software
license, (3)~exhibiting code smells, (4)~containing faults, or (5)~having security vulnerabilities.''

We believe it is important to differentiate the definition of toxic code snippets from the concrete example of toxic code snippets on Stack Overflow since the idea of toxic code snippets can be applied to any other online source (e.g., GitHub) as well.

\rcomment{R2C3: After reading your response, I have a few questions - 1) what are these 100 posts you examined? how were they determined? how was the number 100 determined?}

The 100 posts we examined were the posts containing all the 100 outdated code snippets reported in RQ4, which were discovered by the clone detection tools and manually confirmed by the two investigators.

\rcomment{Also, I am not sure that you answered my comment with your analysis. My point was that some may actually expect the code to be outdated and how do you deal with that, they will usually use newer posts/answers. I am not sure you addressed my comments with your analysis.}

We agree that our analysis may not fully answer your comment because we could not capture what Stack Overflow visitors actually do when they already expect an outdated answer. The most proper method for this may be doing another developer survey. However, our investigation tries to shade light on this by comparing the popularity of the outdated answer and the newer answers based on the number of votes. If Stack Overflow visitors reuse the code (or learn from the answers) and they like the answer, they can reward the answer with an upvote. In our investigation of the 100 outdated answers, we found that only 5 of the newer answers had higher votes than the outdated answers and 99\% of the outdated answers were still marked as accepted. We are aware that this may be affected by the duration that the answers appear on a Stack Overflow post. The newer answers have less visibility compared to the older answers. 

An alternative solution might be directly comparing the number of code snippets reused from the outdated answers to their newer/higher-voted answers in open source projects (e.g., GitHub). 
Nevertheless, if we had performed such comparison, we would still suffer from the same timing issue as the newer answers have shorter windows to be reused than the older ones.

We have added this discussion in the Threats to Validity section \FIXME{Done}.

\rcomment{R2C4: Again, what are the 100 posts (same issues/questions as above arise)? Also, from the table, I would say that only the deprecation posts (15/100) give an indication of outdated code. I am not sure what the rest of the categories in there tell me about the deprecated code.}

The 100 posts are the posts containing the 100 outdated code snippets reported in RQ4. The 6 intents of changes in the 100 outdated code snippets, as shown in the table below (a copy of Table~3 in the previous response letter and Table~20 in the paper), cover \textbf{all} the cases of outdated code. 

\begin{table}[H]
	\centering
	\begin{tabular}{llr}
		\toprule
		Intent & Detail & Amount \\
		\midrule
		Enhancement & Add or update existing features & 64 \\
		Deprecation & Delete dead/deprecated code & 15 \\
		Bug & Fix bugs & 10 \\
		Refactoring & Refactor code for better design & 6 \\
		Coding style & Update to a new style guideline & 3 \\
		Data change & Changes in the data format & 2 \\
		\bottomrule
	\end{tabular}
	\label{tab:intent_outdated}
	\caption{Intents of code changes in the 100 outdated code snippets}
\end{table}

The Deprecation intent partially indicates the cause of outdated code, but not all. It represents the code snippets that are outdated due to the use of deprecated functions or APIs. With this case of function/API deprecation, the code snippets that we analysed contained a few deprecated code statements while the newest version (based on the time we did the analysis) of the same code snippets no longer contain the deprecated part(s). The five other intents represent other cases of a code snippet being outdated. For example, the Bug intent represents a code snippet that previously contained buggy code statements, and were fixed in the newest version. The Refactoring intent represents a code snippet that was different from the newest version due to code refactoring.
To sum up, the table shows that outdated code can be caused by different intents and not all of them are harmful. Outdated code snippets with Bug intent are clearly harmful if they are posted on the Internet (e.g., Stack Overflow). We slightly modified the section explaining this table to be clearer \FIXME{Done}.

\rcomment{R2C5: This is a major issue for me, and I was really hoping that with this revision the authors would update their dataset. By the time this paper hits publication, your data will be 3 years old! In any case, I will leave the decision to the editor, but really, think about a reader of your paper who might question the findings since the data is old. Again, it was not clear to me what the 100 posts and their analysis provide.}

We understand your concern about the age of the data set. However, we do not expect large differences of the findings if we update the data set. The reason is that the Stack Overflow answers are rarely modified. Moreover, updating the dataset would eliminate the possibility to study the evolution of outdated answers. As discussed in RQ4, we used this 2-year gap between the Stack Overflow snapshot that we analysed for clones and the latest version on the web as a chance to study the evolution of the 100 outdated answers. We found that 99\% of the outdated answers are still marked as accepted. While there were a few newer posts in each of the 100 outdated answers, only 5 newer answers obtained higher votes than the outdated ones.

The 100 posts are the posts containing the 100 outdated code snippets reported in RQ4. Their analysis shows the following: 
\begin{enumerate}
	\item Although they are outdated, after 2 years they still have higher votes than other answers or they are still marked as accepted answers.
	\item Only 6 of the outdated answers have newer answers which mitigate or point out the toxicity of the outdated code snippets. 
	\item 47 of the 100 discovered snippets from outdated answers are found in GitHub projects, of which 12 are buggy code. 
\end{enumerate}

Nonetheless, we have added a discussion of the age of the Stack Overflow data set to the Threats to Validity section \FIXME{Done}.

\rcomment{R2C8: Thank you for clarifying the manual analysis part. Can please provide more details on how this manual analysis determined that something actually originated from SO? Perhaps giving an example would help here. Also, who did this verification?}

In the case of cloning direction from Stack Overflow $\rightarrow$ Qualitas
(SQ), we found only one pair with evidence of cloning from Stack Overflow post
ID 698283 to {\small\texttt{POIUtils.java}} in the \textsf{jstock} project (as shown below). 
The Stack Overflow post can be seen at \url{https://stackoverflow.com/questions/698176/determine-correct-method-signature-during-runtime/698283#698283}.
The user who asked the question (i.e., Cheok Yan Cheng) on Stack Overflow is an author of \textsf{jstock} (the author's GitHub profile can be seen from \url{https://github.com/yccheok}). The
question is about determining the right method to call among 7 overloading
methods of {\small\texttt{setCellValue}} during runtime. We could not find
evidence of copying or attribution to Stack Overflow in \textsf{jstock}.
However, considering that the 25 lines of code of
{\small\texttt{findMethodToInvoke}} method depicted in  in Stack Overflow is
very similar to the code in \textsf{jstock} including comments, it is almost
certain that the two code snippets form a clone pair. In addition, the Stack
Overflow answer was posted on March 30, 2009, while the code in
{\small\texttt{POIUtils}} class in \textsf{jstock} was committed to GitHub on
the next day of March 31, 2009. 
The git blame of the file \texttt{POIUtils.java} 
containing the clone can be found from \url{https://github.com/yccheok/jstock/blame/master/src/org/yccheok/jstock/gui/POIUtils.java} (line 99--123).

\begin{figure}[H]
	\begin{lstlisting}
private Method findMethodToInvoke(Object test) {
	Method method = parameterTypeMap.get(test.getClass());
	  if (method != null) {
	    return method;
	  }
	
	// Look for superclasses
	Class<?> x = test.getClass().getSuperclass();
	while (x != null && x != Object.class) {
	  method = parameterTypeMap.get(x);
	  if (method != null) {
	    return method;
	  }
	  x = x.getSuperclass();
	}
	
	// Look for interfaces
	for (Class<?> i : test.getClass().getInterfaces()) {
	  method = parameterTypeMap.get(i);
	  if (method != null) {
	    return method;
	  }
	}
	return null;
}
	\end{lstlisting}
	\caption{The {\small\texttt{findMethodToInvoke}} that is found to be copied from Stack Overflow post 698283 to {\small\texttt{POIUtils}} class in \textsf{jstock}.}
	\label{fig:jstock_code}
\end{figure}

The investigation was performed by the first and
the third author. The two investigators separately went through each clone pair
candidate, looked at the clones, and decided if they are a true positive or a
false positive and classified them into an appropriate pattern by following the steps show in Figure~5. 
After the validation, the results from the two investigators were compared. 
The corresponding discussion can be found in Section~2.2.3 (Manual investigation).

\section{Reviewer 3}

\rcomment{I reviewed the previous submission and am impressed by the improvements to the paper. All of my major concerns are addressed in the new version, and I only have a few minor requests for further improvements:
	
\begin{itemize}
	\item The abstract states that ``We found 100 of them (66\%) to be outdated and potentially harmful for reuse.'' Given the word ``potentially'' this statement is technically true, but the conclusion of the paper is that only 12/100 are actually buggy/harmful. Please revise the abstract to make this clear. The same concern applies to item 3 near the end of Section 1.
\end{itemize}
}

Thank you for the comment. We agree with your suggestion and we changed the statement in the abstract to:

``We found 100 of them (66\%) to be outdated,  \textbf{of which 10 were buggy and harmful for reuse.}''

Moreover, we modified Item 3 in the contributions to be:

``100 of them were found to be outdated\textbf{, of which 10 were buggy code.}'' \FIXME{Done}

\rcomment{
\begin{itemize}
	\item The introduction states that ``We found that the code snippets are usually not authored directly on the Stack Overflow website but copied from another location.'' To me, this is worded too strongly. Please revise.
\end{itemize}
}

Again, we agree with your suggestion. The statement has been changed to ``We found that the code snippets are \textbf{often} not authored directly on the Stack Overflow website but copied from another location.''. \FIXME{Done}

\rcomment{
\begin{itemize}
	\item The new definition of ``toxic code snippets'' is better in terms of content, but the wording is awkward. Please try to express the current definition more clearly/succinctly.
\end{itemize}
}

After incorporating yours and Reviewer 2's comment, we revised the definition of toxic code snippets to be more concise and removed the focus on technical debt as follows.

``Toxic code snippets mean code snippets that, after incorporating into software, degrade the software quality.''

\rcomment{
\begin{itemize}
	\item Regarding the previous Reviewer 1 Comment 5, please add a sentence or two to the paper that mentions point (c), even if only as potential future work.
\end{itemize}
}

We have added a discussion of the point (c) as a footnote in Section 4 as shown below.

``We also found that the Stack Overflow question ID 22262310, which has an outdated answer with a snippet from \texttt{WritableComparator.java} (Figure 1), has a negative score of -1. It would be interesting to see if the negative score for the question impacts the number of views and the trust that people place in the answer to a down-voted question. However, it is out of scope of this study and we leave it as future work.''

\rcomment{Typos, etc:
	\begin{itemize}
		\item In the second sentence of the abstract, ``i.e.'' should be ``e.g.''
		\item On page 14, the sentence ``The intent behind the changes are grouped into six categories as shown in Section 3.4.'' is spurious (as it appears in Section 3.4).
\end{itemize}}

Thank you for spotting them. They are now fixed.

\section{Conclusion}
We would like to thank the reviewers again for their comments.
We have tried our best to address every concern that has been pointed out by the reviewers. This result in several improvements throughout the paper.

% Uncomment in case references are needed
%\bibliographystyle{plain}
\bibliographystyle{spbasic}
\bibliography{references}


\end{document}
