We would like to thank the reviewers for their helpful comments. Please find below the answers.

REVIEWER 1

Question 1: 
Previous studies have found serious problems in code copied from Stack Overflow (SO): An et al. [2] found 62 active Android projects to contain 232 exact copies of snippets on SO (confirmed by developers) and 60 apps violate the original license. 126 snippets were found to potentially migrate from one app to another via SO posts (`code launderingâ€™). Acar et al. [1] examined 139 SO threads used by the participants in four Android security-related tasks and found that 83% of them contain insecure snippets. The same security-related problems are discovered in a random sampling of 200,000 apps (e.g. 98.9% implement insecure hostname verifier similarly to the SO examples).

Question 2:
It is known that clone detection tools report a lot of false/trivial clones. We focus on the important ones. The number of 32,533 analysed clone pairs are not trivial clones according to the clone agreement and clone filtering process (making the manual study feasible). However, they are not the 'total' number of clones between SO and Qualitas. Moreover, one can expect a large number of clones to other projects not in Qualitas.

Question 3:
Classifications of clones by humans are known to be very subjective. However, we expect similar results with different evaluators or a higher number of evaluators.

Question 4:
Although the code is provided as minimum working examples, it is known that the code is used as-is and causing problems in active projects [1,2]. Active maintenance and support provided by SO could mitigate such problems.

REVIEWER 2

Question 1:
We are not aware of any curated set of software that includes non-open source projects. We focused on open source projects to allow replication of our study. 

Question 2:
See Table 11: 79 (QS) and 77 (EX) pairs (snippets) violate the license but acknowledge the source, 202 (UD) pairs violate licenses and have no acknowledgement. 

REVIEWER 3

Question 1:
Disagreed clone pairs are also clones, they just have lower confidence to be true clones. By only inspecting agreed clones, one would miss several true clones due to the tools' differences [61].

Question 3:
We consider them as two clone pairs. In the set of 671 QS+EX+UD clones pairs, a clone in Qualitas is appearing, on average, in 1.2 SO answers.

Question 4:
The automatic classification of boiler plate code is done first (Figure 2). The manual investigation (Figure 4) only classifies boiler plate clones that have been missed by the automatic classification. During the manual investigation, the classification of accidental clones is easier than the boiler plate classification, thus it comes second.

Question 5:
See Table 7: for the manual confirmed pairs, 516/144,064 of snippets with on average 26% of the snippets' code. And for the reported pairs, 14,856/144,064 with an average of 38%. Note that it is restricted to only 111 Qualitas projects and one can expect a large number of clones to other projects.